{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAyAMkaIxd3I"
   },
   "source": [
    "# Importing the dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-6crIx2oi4Pc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.preprocessing as preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzWTMvpC9irH",
    "outputId": "09ab3691-0a19-4a0d-efe9-818cf6f13e1a"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarabic\n",
    "# !pip install langdetect\n",
    "# !pip install nltk\n",
    "\n",
    "# from langdetect import detect\n",
    "# import pyarabic.araby as araby\n",
    "# nltk.download(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66661</th>\n",
       "      <td>0</td>\n",
       "      <td>عرفش ليه كنت كمل وهي مش عجب حدث بطء ممل روي اط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66662</th>\n",
       "      <td>0</td>\n",
       "      <td>لا سحق يكون كنق لنه سيء شي وجد خدم فطر صبح ستي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66663</th>\n",
       "      <td>0</td>\n",
       "      <td>ضعف جدا ولم متع به كل قصه سرد لحل شهد بدن فكر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66664</th>\n",
       "      <td>0</td>\n",
       "      <td>ملة جدا حمد حسن علو فنن وصف عند دقق حد ثني قرء...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66665</th>\n",
       "      <td>0</td>\n",
       "      <td>لن رجع اله مرة خرى قرب بحر كان قدم ولا وجد خدم...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "66661      0  عرفش ليه كنت كمل وهي مش عجب حدث بطء ممل روي اط...\n",
       "66662      0  لا سحق يكون كنق لنه سيء شي وجد خدم فطر صبح ستي...\n",
       "66663      0      ضعف جدا ولم متع به كل قصه سرد لحل شهد بدن فكر\n",
       "66664      0  ملة جدا حمد حسن علو فنن وصف عند دقق حد ثني قرء...\n",
       "66665      0  لن رجع اله مرة خرى قرب بحر كان قدم ولا وجد خدم..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_folder_path=\"/home/youssef/AUC/Spring22/CSCE493002 - Machine Learning/project/datasets\"\n",
    "df = pd.read_csv(dataset_folder_path+'/cleanedText.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64069</th>\n",
       "      <td>0</td>\n",
       "      <td>كبس كان غير نظف رة، شرشف سرر ستر حمم ليء اليوم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46223</th>\n",
       "      <td>0</td>\n",
       "      <td>رغم عجب وسف زيد بدع ءرخ قدر لكن ظل كانت وحد اس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45496</th>\n",
       "      <td>0</td>\n",
       "      <td>حرم سعر خرج دخل له صعب وقع تعب رهق وجد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39719</th>\n",
       "      <td>0</td>\n",
       "      <td>جمل عين وصف فءة ذكر وفه نصح فيد شكل التي يقع ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31607</th>\n",
       "      <td>1</td>\n",
       "      <td>جمل ظرف رغم قلة عدد صفح الا فكر متع دعو ءمل شك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>0</td>\n",
       "      <td>خيب امل سيء لنه ادي فقط</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>1</td>\n",
       "      <td>جيد وقف سير لم تكن كفي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>0</td>\n",
       "      <td>جدد ديو هو ضفة رسم قصد غير تلك مثل رءة لبس ظهر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>0</td>\n",
       "      <td>جنب كان اذا طعت لم يتم وفر نشف وكان علي طلب كل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>0</td>\n",
       "      <td>خيب امل خدم ضعف جدا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66666 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "64069      0  كبس كان غير نظف رة، شرشف سرر ستر حمم ليء اليوم...\n",
       "46223      0  رغم عجب وسف زيد بدع ءرخ قدر لكن ظل كانت وحد اس...\n",
       "45496      0             حرم سعر خرج دخل له صعب وقع تعب رهق وجد\n",
       "39719      0  جمل عين وصف فءة ذكر وفه نصح فيد شكل التي يقع ب...\n",
       "31607      1  جمل ظرف رغم قلة عدد صفح الا فكر متع دعو ءمل شك...\n",
       "...      ...                                                ...\n",
       "41993      0                            خيب امل سيء لنه ادي فقط\n",
       "21243      1                             جيد وقف سير لم تكن كفي\n",
       "45891      0  جدد ديو هو ضفة رسم قصد غير تلك مثل رءة لبس ظهر...\n",
       "42613      0  جنب كان اذا طعت لم يتم وفر نشف وكان علي طلب كل...\n",
       "43567      0                                خيب امل خدم ضعف جدا\n",
       "\n",
       "[66666 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling\n",
    "from  sklearn.utils import shuffle\n",
    "df_shuffled=shuffle(df_copy,random_state=0)\n",
    "df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53326,) (53326, 1) \n",
      "Validation: (6,) (6, 1) \n",
      "Test:  ((13334,), (13334, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Splitting data\n",
    "x=df_shuffled['text']\n",
    "y=np.expand_dims(df_shuffled['label'],axis=1)\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=0.0001,random_state=1)\n",
    "\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"\\nValidation:\",X_val.shape,Y_val.shape,\"\\nTest: \",(X_test.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "tf_x_val=vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53326, 3593) (6, 3593) (13334, 3593)\n"
     ]
    }
   ],
   "source": [
    "print(tf_x_train.shape,tf_x_val.shape,tf_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self,beta1,beta2,alpha,eps=10e-8):\n",
    "#         self.params=params\n",
    "#         self.n_iter=n_iter\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.alpha=alpha\n",
    "        self.eps=eps\n",
    "        self.ms=[]\n",
    "        self.vs=[]\n",
    "        \n",
    "    def reset_params(self,layers):\n",
    "        self.ms=[ \n",
    "                  [np.zeros_like(layer.W,dtype=np.float64), np.zeros_like(layer.b,dtype=np.float64)] \n",
    "                  for layer in layers      \n",
    "                 ]\n",
    "        self.vs=[ \n",
    "                  [np.zeros_like(layer.W,dtype=np.float64), np.zeros_like(layer.b,dtype=np.float64)] \n",
    "                  for layer in layers      \n",
    "                 ]\n",
    "#         print(\"vs:\",self.vs[0][0])\n",
    "        \n",
    "    def update(self,layers,N):\n",
    "#         print(self.vs[0][0])\n",
    "        for i in range(len(layers)):\n",
    "#             print(\"i:\",i)\n",
    "#             print(\"vs part1: \",self.vs[i][0])\n",
    "#             print(\"beta2:\",self.beta2)\n",
    "            self.ms[i][0]= self.beta1*self.ms[i][0]+(1.0-self.beta1)*layers[i].dW\n",
    "            self.ms[i][1]= self.beta1*self.ms[i][1]+(1.0-self.beta1)*layers[i].db\n",
    "            \n",
    "#             print(\"before: vs of\",i,\" = \", self.vs[i][0])\n",
    "            self.vs[i][0]= self.beta2*self.vs[i][0]+(1.0-self.beta2)*np.square(layers[i].dW)\n",
    "            self.vs[i][1]= self.beta2*self.vs[i][1]+(1.0-self.beta2)*np.square(layers[i].db)\n",
    "#             print(\"after: vs of\",i,\" = \", self.vs[i][0])\n",
    "\n",
    "#             print(\"vs:\",self.vs[i][0])\n",
    "#             print(\"eps:\", self.eps)\n",
    "            denDW= np.sqrt((self.vs[i][0] + self.eps))\n",
    "            denB=(np.sqrt((self.vs[i][1] + self.eps)))\n",
    "            \n",
    "            numDW=(-1 * self.alpha * self.ms[i][0])\n",
    "            numB=(-1 * self.alpha * self.ms[i][1])\n",
    "                    \n",
    "            deltaW = np.array(numDW /denDW ,dtype=np.float64)\n",
    "            deltab = np.array( numB/ denB  ,dtype=np.float64)\n",
    "        \n",
    "#             print(\"deltaW\",deltaW)\n",
    "#             print(\"deltab\",deltab)\n",
    "            layers[i].W +=  deltaW/N\n",
    "            layers[i].b +=  deltab/N\n",
    "#             layers[i].W +=  deltaW/np.sqrt(N)\n",
    "#             layers[i].b +=  deltab/np.sqrt(N)\n",
    "        \n",
    "# class GradientDescent:\n",
    "#     def __init__(self,alpha):\n",
    "#         self.alpha=alpha\n",
    "#     def reset_params(self,layers):\n",
    "#         pass\n",
    "#     def update(self,layers,N):\n",
    "#         for i in range(len(layers)):\n",
    "#             # layers[i].dW=layers[i].dW/N\n",
    "#             # layers[i].db=layers[i].db/N\n",
    "#             layers[i].W = layers[i].W - self.alpha * (layers[i].dW/N)\n",
    "#             layers[i].b = layers[i].b - self.alpha * (layers[i].db/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    ### activations\n",
    "    def _relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    def _diff_relu(self,z):\n",
    "        dZ=np.array(z,copy=True)\n",
    "        dZ[dZ<=0]=0\n",
    "        dZ[dZ>0]=1\n",
    "        return dZ\n",
    "    \n",
    "    def _identity(self,z):\n",
    "        return z\n",
    "    \n",
    "    def _identity_diff(self,z):\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "    def _sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-1*z)))\n",
    "\n",
    "    def _diff_sigmoid(self,z):\n",
    "        return self._sigmoid(z)*(1-self._sigmoid(z))\n",
    "    \n",
    "    def _softmax(self,z):\n",
    "        expZ= np.exp(z-np.max(z))\n",
    "        return expZ/expZ.sum(axis=0, keepdims=True)\n",
    "    def _diff_softmax(self,z):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    ###########\n",
    "\n",
    "    def __init__(self,n_input,n_output, activation=\"identity\",name=None):\n",
    "        self.n_output= n_output\n",
    "        self.n_input= n_input\n",
    "        self.name= name\n",
    "        \n",
    "        if activation == \"identity\":\n",
    "            self.activation = self._identity\n",
    "            self.diff_act= self._identity_diff\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = self._sigmoid\n",
    "            self.diff_act= self._diff_sigmoid\n",
    "            \n",
    "        elif activation == \"softmax\":\n",
    "            self.activation=self._softmax\n",
    "            self.diff_act=self._diff_softmax\n",
    "        elif activation ==\"relu\":\n",
    "            self.activation=self._relu\n",
    "            self.diff_act=self._diff_relu\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.W= np.random.randn(self.n_output,self.n_input)*np.sqrt(2/self.n_input)\n",
    "        self.b= np.random.randn(self.n_output,1)*np.sqrt(2/self.n_input)\n",
    "\n",
    "        self.dW= np.zeros_like(self.W)\n",
    "        self.db= np.zeros_like(self.b)\n",
    "        \n",
    "        self.Z= None\n",
    "        self.Ai = None\n",
    "    def print_shapes(self):\n",
    "        print(\"W: \",self.W.shape)\n",
    "        print(\"b: \",self.b.shape)\n",
    "    \n",
    "    def forward(self,Ai): #data dim \n",
    "\n",
    "        z =  np.add((self.W @ Ai),self.b)\n",
    "        A = self.activation(z)\n",
    "\n",
    "        \n",
    "        self.Z = z\n",
    "        self.Ai = Ai\n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self,inp):\n",
    "        \n",
    "#         print(\"input shape: \",end='')\n",
    "#         print(inp.shape)\n",
    "       \n",
    "        act_diff = self.diff_act(self.Z)\n",
    "#         print(\"act_diff shape: \",end='')\n",
    "#         print(act_diff.shape)\n",
    "        \n",
    "        tmp = inp * act_diff\n",
    "#         print(\"tmp shape: \",end='')\n",
    "#         print(tmp.shape)\n",
    "        \n",
    "        bet = tmp @ self.Ai.T # vector of 1s\n",
    "#         print(\"bet shape: \",end='')\n",
    "#         print(bet.shape)\n",
    "        \n",
    "        \n",
    "        e = np.ones((self.Ai.shape[1],1))\n",
    "        db = tmp @ e\n",
    "#         print(\"db shape: \",end='')\n",
    "#         print(db.shape)\n",
    "        self.dW = (self.dW + bet)\n",
    "#         print(\"dw:\",self.dW.shape,\"\\nlen:\",len(self.dW))\n",
    "        self.db = self.db + db\n",
    "        \n",
    "        \n",
    "        return self.W.T @ tmp\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print(\"\\n###################\")\n",
    "        if(self.name):\n",
    "            print(\"name: \",self.name)\n",
    "        print(\"dW: \",self.dW, \"W: \",self.W)\n",
    "    \n",
    "    def zeroing_delta(self):\n",
    "        self.dW= np.zeros_like(self.W)\n",
    "        self.db= np.zeros_like(self.b)\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \n",
    "    ########\n",
    "    ## losses\n",
    "    def _MSE(self,y,yhat):\n",
    "        a=np.square(yhat-y)\n",
    "        a=np.sum(a)\n",
    "        b= 1/(2*y.shape[1])\n",
    "        return a*b\n",
    "\n",
    "    ## diff losses\n",
    "    def _diff_MSE(self,y,yhat,X):\n",
    "        return (yhat-y)\n",
    "    \n",
    "    def _binary_cross_entropy(self,y,yhat):\n",
    "        arr= -(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "        return arr.mean()\n",
    "        \n",
    "    def _diff_binary_cross_entropy(self,y,yhat,X):\n",
    "        dl_dyhat= -(y/(yhat) - (1-y)/(1-yhat))\n",
    "        return dl_dyhat\n",
    " \n",
    "    \n",
    "    #########\n",
    "    \n",
    "    def __init__(self,optimizer=None,loss=\"binary_cross\"):\n",
    "        self.layers = []\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_name=loss\n",
    "        self.initialize_loss()\n",
    "    \n",
    "   \n",
    "    def initialize_loss(self): \n",
    "        if(self.loss_name==\"binary_cross\"):\n",
    "            self.loss=self._binary_cross_entropy\n",
    "            self.loss_diff=self._diff_binary_cross_entropy\n",
    "        elif self.loss_name==\"MSE\":\n",
    "            self.loss=self._MSE\n",
    "            self.loss_diff=self._diff_MSE\n",
    "        \n",
    "    \n",
    " \n",
    "    \n",
    "    def forward(self,x_train):\n",
    "        a=x_train\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self,input):\n",
    "        gd = input\n",
    "        for layer in self.layers[::-1]:\n",
    "            gd = layer.backward(gd)\n",
    "            \n",
    "    def add_layer(self,n_input,n_output, activation=\"identity\",name=None):\n",
    "        self.layers.append(Layer(n_input,n_output, activation=activation,name=name))\n",
    "    \n",
    "    def batch(self,x,y,batch_size):\n",
    "        x= x.copy()\n",
    "        y=y.copy()\n",
    "        reminder= x.shape[0] % batch_size\n",
    "\n",
    "\n",
    "        for i in range(0,x.shape[0],batch_size):\n",
    "            yield (x[i:i+batch_size],y[i:i+batch_size])\n",
    "        \n",
    "        if reminder !=0:\n",
    "            yield (x[x.shape[0]-reminder:],y[x.shape[0]-reminder:] )\n",
    "    \n",
    "    def fit(self, x_train,y_train,validation_data=None,batch_size=32, epochs=5): #data dim is MxN .. M no of examples.. N no of dimension\n",
    "        \n",
    "        M = x_train.shape[0]\n",
    "\n",
    "        no_of_batches= np.ceil(M/batch_size)\n",
    "        if(validation_data):\n",
    "            x_valid=validation_data[0]\n",
    "            y_valid=validation_data[1]\n",
    "        \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            print(\"Epoche {}/{}\".format(i+1,epochs))\n",
    "            self.optimizer.reset_params(self.layers)\n",
    "            batches=self.batch(x_train,y_train,batch_size)\n",
    "            losses=[]\n",
    "            j=0\n",
    "            for cur_x,cur_y in batches:\n",
    "                \n",
    "                cur_x=cur_x.T\n",
    "                cur_y=cur_y.T\n",
    "                \n",
    "                y_hat= self.forward(cur_x)\n",
    "\n",
    "                dl_dyhat = self.loss_diff(cur_y,y_hat,self.layers[-1].Ai)\n",
    "                loss=self.loss(cur_y,y_hat)\n",
    "                \n",
    "                losses.append(loss)\n",
    "\n",
    "                self.backward(dl_dyhat)\n",
    "                \n",
    "                if batch_size==1:\n",
    "                    N= M\n",
    "                else:\n",
    "                    N=cur_x.shape[-1]\n",
    "                \n",
    "                self.optimizer.update(self.layers,N)\n",
    "\n",
    "                # zeroing deltas\n",
    "                for layer in self.layers:\n",
    "                    layer.zeroing_delta()\n",
    "                j+=1\n",
    "                \n",
    "            if validation_data:\n",
    "                y_hat_val = self.forward(x_valid.T)\n",
    "                loss_val= self.loss(y_valid.T,y_hat_val)\n",
    "                print(\"val_loss: {}....\".format(loss_val),end=\" \")\n",
    "                ######\n",
    "                #calc metrics\n",
    "            avg_loss= np.array(losses).mean()\n",
    "            if(avg_loss<0.05):\n",
    "                print(\"Stopping early because loss converged to a small number\")\n",
    "                print(\"losses avg=\",avg_loss)\n",
    "                break\n",
    "            else: print(\"losses avg=\",avg_loss)\n",
    "\n",
    "                \n",
    "\n",
    "        print(\"Finished....\") \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self,x_test): #data dim is NxD .. N no of examples.. D no of dimension\n",
    "#         print(\"x_test:\", x_test.shape)\n",
    "        y_hat= self.forward(x_test.T).T\n",
    "#         return y_hat\n",
    "        print(y_hat)\n",
    "        y_hat[y_hat>0.5]=1\n",
    "        y_hat[y_hat<=0.5]=0\n",
    "        return y_hat\n",
    "                    \n",
    "    def print_weights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(\"layer i= \",i,end=\" \")\n",
    "            self.layers[i].print_weights()\n",
    "    def print_shapes(self):\n",
    "        for layer in self.layers:\n",
    "            layer.print_shapes()\n",
    "    \n",
    "    def save_model(self,path):\n",
    "        model=[self.layers,self.optimizer,self.loss]\n",
    "\n",
    "        file=open(path,\"wb\")\n",
    "        print(\"dumped model: \",model)\n",
    "\n",
    "        pickle.dump(model,file)\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    def load_model(self,path):\n",
    "        file=open(path,\"rb\")\n",
    "\n",
    "        model=pickle.load(file)\n",
    "\n",
    "        file.close()\n",
    "        print(\"loaded model: \",model)\n",
    "        \n",
    "        self.layers,self.optimizer,self.loss=model\n",
    "        self.initialize_loss()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53326, 3593)\n",
      "(13334, 3593)\n",
      "(6, 3593)\n",
      "(53326, 1)\n",
      "(13334, 1)\n"
     ]
    }
   ],
   "source": [
    "print(tf_x_train.shape)\n",
    "print(tf_x_test.shape)\n",
    "print(tf_x_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adam= AdamOptimizer(beta1 = 0.9,beta2 = 0.99,alpha=0.1,eps=0.001)\n",
    "nn = NN(optimizer=adam)\n",
    "\n",
    "nn.add_layer(tf_x_train.shape[1],64,activation=\"relu\",name=\"l1\")\n",
    "nn.add_layer(64,32,activation = \"relu\",name=\"l2\")\n",
    "nn.add_layer(32,8,activation = \"relu\",name=\"l4\")\n",
    "nn.add_layer(8,1,activation = \"sigmoid\",name=\"l5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model:  [[<__main__.Layer object at 0x7f3d45e0cf70>, <__main__.Layer object at 0x7f3d45f44d60>, <__main__.Layer object at 0x7f3d45f44eb0>, <__main__.Layer object at 0x7f3d4d0444c0>], <__main__.AdamOptimizer object at 0x7f3d5211dbe0>, <bound method NN._binary_cross_entropy of <__main__.NN object at 0x7f3d45ec0760>>]\n"
     ]
    }
   ],
   "source": [
    "nn.fit(tf_x_train,Y_train,validation_data=[tf_x_val,Y_val],batch_size=32,epochs=20)\n",
    "# nn.load_model(\"modelDump.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.97813280e-03]\n",
      " [5.10644309e-03]\n",
      " [9.99935068e-01]\n",
      " ...\n",
      " [2.57242167e-04]\n",
      " [9.93724189e-01]\n",
      " [9.97392866e-01]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=nn.predict(tf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([6151, 7183]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "np.unique(y_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.preprocessing as preprocess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.83      0.80      6146\n",
      "         1.0       0.85      0.79      0.82      7188\n",
      "\n",
      "    accuracy                           0.81     13334\n",
      "   macro avg       0.81      0.81      0.81     13334\n",
      "weighted avg       0.81      0.81      0.81     13334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y_test\n",
    "print(classification_report(y_pred,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped model:  [[<__main__.Layer object at 0x7f7eb69300a0>, <__main__.Layer object at 0x7f7ebc557280>, <__main__.Layer object at 0x7f7eb83b3d90>, <__main__.Layer object at 0x7f7ebc557ac0>], <__main__.AdamOptimizer object at 0x7f7eb69302b0>, <bound method NN._binary_cross_entropy of <__main__.NN object at 0x7f7eb6930160>>]\n"
     ]
    }
   ],
   "source": [
    "#dump model\n",
    "nn.save_model(\"modelDump.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"tfidfVectorizerDump.joblib\",\"wb\")\n",
    "pickle.dump(file,vectorizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "detect_language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
