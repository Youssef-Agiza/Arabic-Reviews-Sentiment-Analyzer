{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAyAMkaIxd3I"
   },
   "source": [
    "# Importing the dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-6crIx2oi4Pc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.preprocessing as preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzWTMvpC9irH",
    "outputId": "09ab3691-0a19-4a0d-efe9-818cf6f13e1a"
   },
   "outputs": [],
   "source": [
    "# !pip install pyarabic\n",
    "# !pip install langdetect\n",
    "# !pip install nltk\n",
    "\n",
    "# from langdetect import detect\n",
    "# import pyarabic.araby as araby\n",
    "# nltk.download(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_folder_path=\"/home/youssef/AUC/Spring22/CSCE493002 - Machine Learning/project/repo/datasets\"\n",
    "df = pd.read_csv(dataset_folder_path+'/cleanedText.csv')\n",
    "# df = pd.read_csv(dataset_folder_path+'/ar_reviews_100k.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[\"text\"]=df_copy['text'].values.astype('U')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk import word_tokenize\n",
    "import pyarabic.araby as araby\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self,vectorizer_path=None):\n",
    "        self.vectorizer=None \n",
    "        if vectorizer_path:\n",
    "            self.load_tfidf_vectorizer(vectorizer_path)\n",
    "    \n",
    "    def load_tfidf_vectorizer(self,path):\n",
    "        file=open(path,'rb')\n",
    "        self.vectorizer=pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    def save_tfidf_vectorizer(self,path):\n",
    "        if not self.vectorizer:\n",
    "            return None\n",
    "        file=open(path,\"wb\")\n",
    "        pickle.dump(file,self.vectorizer)\n",
    "        file.close()\n",
    "    \n",
    "    def preprocess(self,sentence,vectorizer_path=\"\"):\n",
    "#         if not self.vectorizer and vectorizer_path==\"\":\n",
    "#             return None\n",
    "\n",
    "        # if not self.detect_lang(sentence):\n",
    "        #     print(\"sentence Not arabic!\")\n",
    "        #     return None\n",
    "        \n",
    "        result=self.stem(sentence)\n",
    "        result=self.normalize(result)\n",
    "        result=self.remove_redundant_words(result)\n",
    "\n",
    "\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def detect_lang(self,sentence):\n",
    "        try:\n",
    "            print(\"\\n\\nhere\\n\\n\")\n",
    "            language = detect(sentence)\n",
    "            print(\"\\n\\nhere2\\n\\n\")\n",
    "\n",
    "            if language != 'ar':\n",
    "                return False\n",
    "        except:\n",
    "                return False\n",
    "        print(\"returning true\")\n",
    "        return True\n",
    "\n",
    "    def stem(self,sentence):\n",
    "        st = ISRIStemmer()\n",
    "        stemmed_sentence=\"\"\n",
    "        for a in word_tokenize(sentence):\n",
    "            stemmed=st.stem(a)\n",
    "            stemmed_sentence+=(stemmed+\" \")\n",
    "        return stemmed_sentence\n",
    "    \n",
    "    def normalize(self,sentence):\n",
    "        normalized=sentence\n",
    "        normalized=araby.strip_tashkeel(normalized)\n",
    "        normalized= araby.strip_tatweel(normalized)\n",
    "        normalized=araby.normalize_hamza(normalized)\n",
    "        return normalized\n",
    "    \n",
    "    \n",
    "\n",
    "    def remove_redundant_words(self,sentence):\n",
    "        stop_words=['من','على','عن','ب','ك','ل','فى','و','ان','هذا','او','كتب','...','.','','الى','فيه','انه','قبل','//','..','،',':',\"؟\",'/']\n",
    "        words=sentence.split()\n",
    "        resultWords= [word for word in words if word not in stop_words]\n",
    "        return ' '.join(resultWords)\n",
    "    \n",
    "    def convert_to_tfidf(self,sentence):\n",
    "        if not self.vectorizer:\n",
    "            return None\n",
    "        print(self.vectorizer)\n",
    "        return self.vectorizer.transform([sentence])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "from  sklearn.utils import shuffle\n",
    "df_shuffled=shuffle(df_copy,random_state=0)\n",
    "df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Splitting data\n",
    "x=df_shuffled['text']\n",
    "y=np.expand_dims(df_shuffled['label'],axis=1)\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=0.25,random_state=1)\n",
    "\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"\\nValidation:\",X_val.shape,Y_val.shape,\"\\nTest: \",(X_test.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "tf_x_val=vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_file_path=\"tfidfVectorizerDump.joblib\"\n",
    "with open(vectorizer_file_path,\"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tf_x_train.shape,tf_x_val.shape,tf_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "\n",
    "# iterate through each sentence in the file\n",
    "def toknize_text(sentence):\n",
    "    data = []\n",
    "    for i in sent_tokenize(sentence):\n",
    "        temp = []\n",
    "\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "\n",
    "        data=temp\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_shuffled['toknized']=df_shuffled['text'].apply(toknize_text)\n",
    "df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toknized_text=df_shuffled['toknized']\n",
    "preprocess=Preprocessor().preprocess\n",
    "words=['انثى','ملك','ذكر']\n",
    "words=[preprocess(word) for word in words]\n",
    "pos=[words[1],words[0]]\n",
    "negs=[words[2]]\n",
    "print(negs)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(toknized_text, min_count = 1, vector_size = 300, window = 5)\n",
    "model1.wv.most_similar(positive=pos,negative=negs,topn=10)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(toknized_text, min_count = 1, vector_size = 300, window = 5, sg = 1,workers=20)\n",
    "print(model2.wv.most_similar(positive=pos,negative=negs,topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.get_vector('ملك')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Weight to the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    try:\n",
    "        language = detect(row[1])\n",
    "        if language != 'ar':\n",
    "          row[\"text\"]=\"-1\"\n",
    "    except:\n",
    "        language = \"error\"\n",
    "        print(\"Row \",index,\" does not contain Arabic words and will be removed\")\n",
    "        row[\"text\"]=\"-1\"\n",
    "        #print(row[\"text\"])\n",
    "\n",
    "df = df[df['text']!=\"-1\"] #remove non-arabic text\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(sentence):\n",
    "    if(type(sentence)==type(float(0.1))):\n",
    "        print(sentence)\n",
    "#   print(type(sentence))\n",
    "#   freq=dict()\n",
    "#   for word in sentence:\n",
    "#       if word in freq:\n",
    "#         freq[word]+=1\n",
    "#       else:\n",
    "#         freq[word]=1\n",
    "#   return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq']=df['text'].apply(get_freq)\n",
    "# df['freq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelEmb1 (row):\n",
    "    #print(row.name)\n",
    "    freqDict=row[\"freq\"]\n",
    "\n",
    "    sentence = np.array(list(freqDict.keys()))\n",
    "    \n",
    "#     print(row.name)\n",
    "    arr= [arrOfEmbeddings[ind]*freqDict[sentence[ind]]#tfidfDF[sentence[ind]][row.name] if sentence [ind] in tfidfDF.columns else 0#freqDict[sentence[ind]]\n",
    "          for ind in range(len(sentence)) \n",
    "          if sentence[ind] in unique_dict.keys()]\n",
    "    \n",
    "    totalFreq = sum(freqDict.values())\n",
    "    \n",
    "    if len(arr) ==0:\n",
    "       arr = list(np.zeros(300))\n",
    "        \n",
    "    arr=np.array(arr)\n",
    "    \n",
    "    vectorSum = np.sum(arr,axis=0)\n",
    "    \n",
    "    if (totalFreq == 0):\n",
    "       vectorSum = list(np.zeros(300))\n",
    "    else: \n",
    "       vectorSum = vectorSum * 1/totalFreq \n",
    "    return vectorSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self,beta1,beta2,alpha,eps=10e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.alpha=alpha\n",
    "        self.eps=eps\n",
    "        self.ms=[]\n",
    "        self.vs=[]\n",
    "        \n",
    "    def reset_params(self,layers):\n",
    "        self.ms=[ \n",
    "                  [np.zeros_like(layer.W,dtype=np.float64), np.zeros_like(layer.b,dtype=np.float64)] \n",
    "                  for layer in layers      \n",
    "                 ]\n",
    "        self.vs=[ \n",
    "                  [np.zeros_like(layer.W,dtype=np.float64), np.zeros_like(layer.b,dtype=np.float64)] \n",
    "                  for layer in layers      \n",
    "                 ]\n",
    "        \n",
    "    def update(self,layers,N):\n",
    "        for i in range(len(layers)):\n",
    "            self.ms[i][0]= self.beta1*self.ms[i][0]+(1.0-self.beta1)*layers[i].dW\n",
    "            self.ms[i][1]= self.beta1*self.ms[i][1]+(1.0-self.beta1)*layers[i].db\n",
    "            \n",
    "            self.vs[i][0]= self.beta2*self.vs[i][0]+(1.0-self.beta2)*np.square(layers[i].dW)\n",
    "            self.vs[i][1]= self.beta2*self.vs[i][1]+(1.0-self.beta2)*np.square(layers[i].db)\n",
    "\n",
    "            denDW= np.sqrt((self.vs[i][0] + self.eps))\n",
    "            denB=(np.sqrt((self.vs[i][1] + self.eps)))\n",
    "            \n",
    "            numDW=(-1 * self.alpha * self.ms[i][0])\n",
    "            numB=(-1 * self.alpha * self.ms[i][1])\n",
    "                    \n",
    "            deltaW = np.array(numDW /denDW ,dtype=np.float64)\n",
    "            deltab = np.array( numB/ denB  ,dtype=np.float64)\n",
    "        \n",
    "            layers[i].W +=  deltaW/N\n",
    "            layers[i].b +=  deltab/N\n",
    "        \n",
    "class GradientDescent:\n",
    "    def __init__(self,alpha):\n",
    "        self.alpha=alpha\n",
    "    def reset_params(self,layers):\n",
    "        pass\n",
    "    def update(self,layers,N):\n",
    "        for i in range(len(layers)):\n",
    "            # layers[i].dW=layers[i].dW/N\n",
    "            # layers[i].db=layers[i].db/N\n",
    "            layers[i].W = layers[i].W - self.alpha * (layers[i].dW/N)\n",
    "            layers[i].b = layers[i].b - self.alpha * (layers[i].db/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    ### activations\n",
    "    def _relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    def _diff_relu(self,z):\n",
    "        dZ=np.array(z,copy=True)\n",
    "        dZ[dZ<=0]=0\n",
    "        dZ[dZ>0]=1\n",
    "        return dZ\n",
    "    \n",
    "    def _identity(self,z):\n",
    "        return z\n",
    "    \n",
    "    def _identity_diff(self,z):\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "    def _sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-1*z)))\n",
    "\n",
    "    def _diff_sigmoid(self,z):\n",
    "        return self._sigmoid(z)*(1-self._sigmoid(z))\n",
    "    \n",
    "    def _softmax(self,z):\n",
    "        expZ= np.exp(z-np.max(z))\n",
    "        return expZ/expZ.sum(axis=0, keepdims=True)\n",
    "    def _diff_softmax(self,z):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    ###########\n",
    "\n",
    "    def __init__(self,n_input,n_output, activation=\"identity\",name=None):\n",
    "        self.n_output= n_output\n",
    "        self.n_input= n_input\n",
    "        self.name= name\n",
    "        \n",
    "        if activation == \"identity\":\n",
    "            self.activation = self._identity\n",
    "            self.diff_act= self._identity_diff\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = self._sigmoid\n",
    "            self.diff_act= self._diff_sigmoid\n",
    "            \n",
    "        elif activation == \"softmax\":\n",
    "            self.activation=self._softmax\n",
    "            self.diff_act=self._diff_softmax\n",
    "        elif activation ==\"relu\":\n",
    "            self.activation=self._relu\n",
    "            self.diff_act=self._diff_relu\n",
    "            \n",
    "        self.reset_params()\n",
    "            \n",
    "        \n",
    "    def reset_params(self): \n",
    "        self.W= np.random.randn(self.n_output,self.n_input)*np.sqrt(2/self.n_input)\n",
    "        self.b= np.random.randn(self.n_output,1)*np.sqrt(2/self.n_input)\n",
    "\n",
    "        self.dW= np.zeros_like(self.W)\n",
    "        self.db= np.zeros_like(self.b)\n",
    "        \n",
    "        self.Z= None\n",
    "        self.Ai = None\n",
    "    def print_shapes(self):\n",
    "        print(\"W: \",self.W.shape)\n",
    "        print(\"b: \",self.b.shape)\n",
    "    \n",
    "    def forward(self,Ai): #data dim \n",
    "\n",
    "        z =  np.add((self.W @ Ai),self.b)\n",
    "        A = self.activation(z)\n",
    "\n",
    "        \n",
    "        self.Z = z\n",
    "        self.Ai = Ai\n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self,inp):\n",
    "        \n",
    "       \n",
    "        act_diff = self.diff_act(self.Z)\n",
    "        \n",
    "        tmp = inp * act_diff\n",
    "        \n",
    "        bet = tmp @ self.Ai.T # vector of 1s\n",
    "        \n",
    "        \n",
    "        e = np.ones((self.Ai.shape[1],1))\n",
    "        db = tmp @ e\n",
    "\n",
    "        self.dW = (self.dW + bet)\n",
    "        self.db = self.db + db\n",
    "        \n",
    "        \n",
    "        return self.W.T @ tmp\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print(\"\\n###################\")\n",
    "        if(self.name):\n",
    "            print(\"name: \",self.name)\n",
    "        print(\"dW: \",self.dW, \"W: \",self.W)\n",
    "    \n",
    "    def zeroing_delta(self):\n",
    "        self.dW= np.zeros_like(self.W)\n",
    "        self.db= np.zeros_like(self.b)\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \n",
    "    ########\n",
    "    ## losses\n",
    "    def _MSE(self,y,yhat):\n",
    "        a=np.square(yhat-y)\n",
    "        a=np.sum(a)\n",
    "        b= 1/(2*y.shape[1])\n",
    "        return a*b\n",
    "\n",
    "    ## diff losses\n",
    "    def _diff_MSE(self,y,yhat,X):\n",
    "        return (yhat-y)\n",
    "    \n",
    "    def _binary_cross_entropy(self,y,yhat):\n",
    "        arr= -(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "        return arr.mean()\n",
    "        \n",
    "    def _diff_binary_cross_entropy(self,y,yhat,X):\n",
    "        dl_dyhat= -(y/(yhat) - (1-y)/(1-yhat))\n",
    "        return dl_dyhat\n",
    " \n",
    "    \n",
    "    #########\n",
    "    \n",
    "    def __init__(self,optimizer=None,loss=\"binary_cross\"):\n",
    "        self.layers = []\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_name=loss\n",
    "        self.initialize_loss()\n",
    "    \n",
    "   \n",
    "    def initialize_loss(self): \n",
    "        if(self.loss_name==\"binary_cross\"):\n",
    "            self.loss=self._binary_cross_entropy\n",
    "            self.loss_diff=self._diff_binary_cross_entropy\n",
    "        elif self.loss_name==\"MSE\":\n",
    "            self.loss=self._MSE\n",
    "            self.loss_diff=self._diff_MSE\n",
    "        \n",
    "    \n",
    "    def reset_layers(self):\n",
    "            for layer in self.layers:\n",
    "                layer.reset_params()\n",
    "    \n",
    "    def forward(self,x_train):\n",
    "        a=x_train\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self,input):\n",
    "        gd = input\n",
    "        for layer in self.layers[::-1]:\n",
    "            gd = layer.backward(gd)\n",
    "            \n",
    "    def add_layer(self,n_input,n_output, activation=\"identity\",name=None):\n",
    "        self.layers.append(Layer(n_input,n_output, activation=activation,name=name))\n",
    "    \n",
    "    def batch(self,x,y,batch_size):\n",
    "        x= x.copy()\n",
    "        y=y.copy()\n",
    "        reminder= x.shape[0] % batch_size\n",
    "\n",
    "\n",
    "        for i in range(0,x.shape[0],batch_size):\n",
    "            yield (x[i:i+batch_size],y[i:i+batch_size])\n",
    "        \n",
    "        if reminder !=0:\n",
    "            yield (x[x.shape[0]-reminder:],y[x.shape[0]-reminder:] )\n",
    "    \n",
    "    def fit(self, x_train,y_train,validation_data=None,batch_size=32, epochs=5): #data dim is MxN .. M no of examples.. N no of dimension\n",
    "        \n",
    "        M = x_train.shape[0]\n",
    "\n",
    "        no_of_batches= np.ceil(M/batch_size)\n",
    "        if(validation_data):\n",
    "            x_valid=validation_data[0]\n",
    "            y_valid=validation_data[1]\n",
    "        \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            print(\"Epoche {}/{}\".format(i+1,epochs))\n",
    "            self.optimizer.reset_params(self.layers)\n",
    "            batches=self.batch(x_train,y_train,batch_size)\n",
    "            losses=[]\n",
    "            j=0\n",
    "            for cur_x,cur_y in batches:\n",
    "                \n",
    "                cur_x=cur_x.T\n",
    "                cur_y=cur_y.T\n",
    "                \n",
    "                y_hat= self.forward(cur_x)\n",
    "\n",
    "                dl_dyhat = self.loss_diff(cur_y,y_hat,self.layers[-1].Ai)\n",
    "                loss=self.loss(cur_y,y_hat)\n",
    "                \n",
    "                losses.append(loss)\n",
    "\n",
    "                self.backward(dl_dyhat)\n",
    "                \n",
    "                if batch_size==1:\n",
    "                    N= M\n",
    "                else:\n",
    "                    N=cur_x.shape[-1]\n",
    "                \n",
    "                self.optimizer.update(self.layers,N)\n",
    "\n",
    "                # zeroing deltas\n",
    "                for layer in self.layers:\n",
    "                    layer.zeroing_delta()\n",
    "                j+=1\n",
    "                \n",
    "            if validation_data:\n",
    "                y_hat_val = self.forward(x_valid.T)\n",
    "                loss_val= self.loss(y_valid.T,y_hat_val)\n",
    "                print(\"val_loss: {}....\".format(loss_val),end=\" \")\n",
    "                ######\n",
    "                #calc metrics\n",
    "            avg_loss= np.array(losses).mean()\n",
    "            if(avg_loss<0.05):\n",
    "                print(\"Stopping early because loss converged to a small number\")\n",
    "                print(\"losses avg=\",avg_loss)\n",
    "                break\n",
    "            else: print(\"losses avg=\",avg_loss)\n",
    "\n",
    "                \n",
    "\n",
    "        print(\"Finished....\") \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self,x_test): #data dim is NxD .. N no of examples.. D no of dimension\n",
    "        y_hat= self.forward(x_test.T).T\n",
    "\n",
    "        y_hat[y_hat>0.5]=1\n",
    "        y_hat[y_hat<=0.5]=0\n",
    "        return y_hat\n",
    "                    \n",
    "    def print_weights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(\"layer i= \",i,end=\" \")\n",
    "            self.layers[i].print_weights()\n",
    "    def print_shapes(self):\n",
    "        for layer in self.layers:\n",
    "            layer.print_shapes()\n",
    "    \n",
    "    def save_model(self,path):\n",
    "        model=[self.layers,self.optimizer,self.loss]\n",
    "\n",
    "        file=open(path,\"wb\")\n",
    "        print(\"dumped model: \",model)\n",
    "\n",
    "        pickle.dump(model,file)\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    def load_model(self,path):\n",
    "        file=open(path,\"rb\")\n",
    "\n",
    "        model=pickle.load(file)\n",
    "\n",
    "        file.close()\n",
    "        print(\"loaded model: \",model)\n",
    "        \n",
    "        self.layers,self.optimizer,self.loss=model\n",
    "        self.initialize_loss()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tf_x_train.shape)\n",
    "print(tf_x_test.shape)\n",
    "print(tf_x_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adam= AdamOptimizer(beta1 = 0.9,beta2 = 0.99,alpha=0.1,eps=0.001)\n",
    "nn = NN(optimizer=adam)\n",
    "\n",
    "nn.add_layer(tf_x_train.shape[1],64,activation=\"relu\",name=\"l1\")\n",
    "nn.add_layer(64,32,activation = \"relu\",name=\"l2\")\n",
    "nn.add_layer(32,8,activation = \"relu\",name=\"l4\")\n",
    "nn.add_layer(8,1,activation = \"sigmoid\",name=\"l5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit(tf_x_train,Y_train,validation_data=[tf_x_val,Y_val],batch_size=32,epochs=1)\n",
    "# nn.load_model(\"modelDump.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=nn.predict(tf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "np.unique(y_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test\n",
    "print(classification_report(y_pred,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dump model\n",
    "nn.save_model(\"modelDump.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"tfidfVectorizerDump.joblib\",\"wb\")\n",
    "pickle.dump(file,vectorizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "detect_language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
